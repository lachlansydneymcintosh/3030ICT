{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjFuaP6dw6Mf"
      },
      "source": [
        "# 01-TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMY1ezCkw6Mh"
      },
      "source": [
        "We will here compute the TF-IDF on a corpus of newspaper headlines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYyfjor_w6Mh"
      },
      "source": [
        "Begin by importing needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SH3thNJkw6Mh"
      },
      "outputs": [],
      "source": [
        "# import needed libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znw53-U5w6Mi"
      },
      "source": [
        "Import the data into the file *headlines.csv*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qo1WAGyCw6Mi"
      },
      "outputs": [],
      "source": [
        "# TODO: Load the dataset\n",
        "df = pd.read_csv('headlines.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSFGULy1w6Mi"
      },
      "source": [
        "As usual, check the dataset basic information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1v0oSaHw6Mi",
        "outputId": "1cca619f-f7e4-4190-f4f4-c1c65cb32440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   publish_date                                      headline_text\n",
            "0      20170721  algorithms can make decisions on behalf of fed...\n",
            "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
            "2      20170721                           a rural mural in thallan\n",
            "3      20170721  australia church risks becoming haven for abusers\n",
            "4      20170721  australian company usgfx embroiled in shanghai...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1999 entries, 0 to 1998\n",
            "Data columns (total 2 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   publish_date   1999 non-null   int64 \n",
            " 1   headline_text  1999 non-null   object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 31.4+ KB\n"
          ]
        }
      ],
      "source": [
        "# TODO: Have a look at the data\n",
        "print(df.head(5))\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ko6GBdtw6Mj"
      },
      "source": [
        "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
        "\n",
        "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMocKNvmw6Mj",
        "outputId": "1e287b02-4089-4519-dc76-194d23052490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         [algorithm, make, decis, behalf, feder, minist]\n",
              "1       [andrew, forrest, fmg, appeal, pilbara, nativ,...\n",
              "2                                 [rural, mural, thallan]\n",
              "3                  [australia, church, risk, becom, abus]\n",
              "4       [australian, compani, usgfx, embroil, shanghai...\n",
              "                              ...                        \n",
              "1994    [constitut, avenu, win, top, prize, act, archi...\n",
              "1995                         [dark, mofo, number, crunch]\n",
              "1996    [david, petraeu, say, australia, must, firm, s...\n",
              "1997    [driverless, car, australia, face, challeng, r...\n",
              "1998               [drug, compani, criticis, price, hike]\n",
              "Name: stemmed, Length: 1999, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# TODO: Perform preprocessing\n",
        "# import needed modules\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize\n",
        "df['tokens'] = df.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
        "\n",
        "# Remove punctuation\n",
        "df['alpha'] = df['tokens'].apply(lambda x: [item for item in x if item.isalpha()])\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = stopwords.words(\"english\")\n",
        "# df['stop'] = df['alpha'].apply(lambda x: [item for item in x if item not in stop])\n",
        "df['stop'] = df['alpha'].apply(lambda x: [item for item in x if item not in stopwords.words(\"english\")])\n",
        "\n",
        "# Stem\n",
        "stemmer = PorterStemmer()\n",
        "df['stemmed'] = df['stop'].apply(lambda x: [stemmer.stem(item) for item in x])\n",
        "df['stemmed']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZRtT6Pcw6Mj"
      },
      "source": [
        "Compute now the Bag of Words for our data, using scikit-learn.\n",
        "\n",
        "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgrYzXcUw6Mj",
        "outputId": "e4776c8b-68ea-4acb-f1ce-5c09a1afba00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1999, 4165)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# TODO: Compute the BOW of the preprocessed data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase=False, analyzer=lambda x: x)\n",
        "BOW = vectorizer.fit_transform(df['stemmed']).toarray()\n",
        "BOW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cexk3H-gw6Mj"
      },
      "source": [
        "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3WNvvk7w6Mj"
      },
      "source": [
        "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XbkGC3Iw6Mj",
        "outputId": "5ce0a612-f8cf-4cfa-d1c8-1aa2795dddfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.08333333, 0.09090909, 0.1       , 0.11111111,\n",
              "       0.125     , 0.14285714, 0.16666667, 0.18181818, 0.2       ,\n",
              "       0.22222222, 0.25      , 0.28571429, 0.33333333, 0.4       ,\n",
              "       0.5       , 1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# TODO: Compute the TF using the BOW\n",
        "TF = pd.DataFrame(data=BOW, columns=vectorizer.get_feature_names_out())\n",
        "TF = TF.divide(TF.sum(axis=1), axis=0)\n",
        "np.unique(TF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdX44VJzw6Mj",
        "outputId": "64ac2366-75ff-4dff-9993-bb26f8d6dfc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.28291422, 3.36629583, 3.44151925, 3.53995932, 3.57505064,\n",
              "       3.70858204, 3.79373984, 3.83920222, 3.91152288, 3.96281617,\n",
              "       4.04505427, 4.10389477, 4.13466643, 4.16641513, 4.19920495,\n",
              "       4.2331065 , 4.26819782, 4.30456547, 4.3423058 , 4.38152651,\n",
              "       4.4223485 , 4.46490812, 4.50935988, 4.5558799 , 4.60467006,\n",
              "       4.65596336, 4.71003058, 4.76718899, 4.82781361, 4.89235213,\n",
              "       4.961345  , 5.03545298, 5.11549568, 5.20250706, 5.29781724,\n",
              "       5.40317776, 5.52096079, 5.65449219, 5.80864287, 5.99096442,\n",
              "       6.21410797, 6.50179005, 6.90725515, 7.60040233])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# TODO: Compute the IDF\n",
        "IDF = pd.DataFrame(data=BOW, columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "IDF[IDF>1] = 1\n",
        "\n",
        "IDF = np.log(len(IDF)/IDF.sum(axis=0))\n",
        "\n",
        "np.unique(IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH0Tq7IWw6Mk"
      },
      "source": [
        "Compute finally the TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qn7YLNtJw6Mk"
      },
      "outputs": [],
      "source": [
        "# TODO: compute the TF-IDF\n",
        "tf_idf = TF * IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkMtH9pIw6Mk"
      },
      "source": [
        "What are the 10 words with the highest and lowest TF-IDF on average?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7POhg6Ww6Mk",
        "outputId": "05782319-1a27-4c4c-ac6d-6fff095d623f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lowest words: gcfc    0.633367\n",
            "geel    0.633367\n",
            "gw      0.633367\n",
            "haw     0.633367\n",
            "melb    0.633367\n",
            "coll    0.633367\n",
            "adel    0.633367\n",
            "syd     0.633367\n",
            "nmfc    0.633367\n",
            "cold    0.690456\n",
            "dtype: float64\n",
            "highest words: peacemak     7.600402\n",
            "pump         6.907255\n",
            "date         3.800201\n",
            "puffbal      3.800201\n",
            "superannu    3.800201\n",
            "mongolian    3.800201\n",
            "loophol      3.800201\n",
            "rig          3.800201\n",
            "aquapon      3.800201\n",
            "mous         3.800201\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
        "print('lowest words:', tf_idf.max(axis=0).sort_values()[:10])\n",
        "print('highest words:', tf_idf.max(axis=0).sort_values(ascending=False)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK3Gkr1Vw6Mk"
      },
      "source": [
        "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UhzbED-8w6Mk"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute the TF-IDF using scikit learn\n",
        "# Import the module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Instantiate the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(lowercase=False, analyzer=lambda x: x)\n",
        "\n",
        "# Compute the TF-IDF\n",
        "tf_idf = vectorizer.fit_transform(df['stemmed']).toarray()\n",
        "tf_idf = pd.DataFrame(data=tf_idf, columns=vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cRoCiaGw6Mk"
      },
      "source": [
        "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzmEDajRw6Mk",
        "outputId": "5bcb2bc8-cfac-44a9-b0d7-6aab46dc01ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lowest words: coll     0.305258\n",
            "gw       0.305258\n",
            "nmfc     0.305258\n",
            "adel     0.305258\n",
            "melb     0.305258\n",
            "syd      0.305258\n",
            "haw      0.305258\n",
            "geel     0.305258\n",
            "gcfc     0.305258\n",
            "fabio    0.322574\n",
            "dtype: float64\n",
            "highest words: peacemak     1.000000\n",
            "pump         1.000000\n",
            "mongolian    0.831769\n",
            "financ       0.803629\n",
            "employ       0.795060\n",
            "aquapon      0.794899\n",
            "date         0.794899\n",
            "travel       0.788050\n",
            "rig          0.786813\n",
            "mosul        0.779137\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
        "print('lowest words:', tf_idf.max(axis=0).sort_values()[:10])\n",
        "print('highest words:', tf_idf.max(axis=0).sort_values(ascending=False)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zBrw9RDw6Mk"
      },
      "source": [
        "**Do you have the same words? How do you explain it?**\n",
        "\n",
        "There are none of the same words within the lowest and highest word brackets, this is likely because there is more than twenty words thus making it impossible for overlap in this case."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}